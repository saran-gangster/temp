{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(a,b):\n",
    "    a = int(a.strip(\"<>\"))\n",
    "    b = int(b.strip(\"<>\"))\n",
    "    if a<b:\n",
    "        return 0\n",
    "    if a==b:\n",
    "        return 1\n",
    "    if a>b:\n",
    "        return 2\n",
    "    assert False\n",
    "    \n",
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "\n",
    "num_entities = 1000\n",
    "entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "vocab = vocab + entities\n",
    "ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "num_attributes = 20\n",
    "attributes = [\"<attr_{}>\".format(i) for i in range(num_attributes)]\n",
    "vocab = vocab + attributes\n",
    "ind2attribute, attribute2ind = build_dicts(attributes)\n",
    "\n",
    "num_vals_per_attr = 20  # values range from [0, num_vals_per_attr-1]\n",
    "values = [\"<{}>\".format(i) for i in range(num_vals_per_attr)]\n",
    "vocab = vocab + values\n",
    "\n",
    "# randomly assign values to people's attributes\n",
    "atomic_KB = np.random.randint(low=0, high=num_vals_per_attr, size=(num_entities, num_attributes))     #  [entity id, attribute id] -> value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "    \n",
    "def split(arr, ratio):\n",
    "    train, test = [], []\n",
    "    rand_inds = np.random.choice(len(arr), round(ratio*len(arr)), replace=False).tolist()\n",
    "    for i in range(len(arr)):\n",
    "        if i in rand_inds:\n",
    "            train.append(arr[i])\n",
    "        else:\n",
    "            test.append(arr[i])\n",
    "    return [train, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "\n",
    "comp_q_tokens = attributes\n",
    "comp2labels = dict()\n",
    "for comp_q_token in comp_q_tokens:\n",
    "    comp2labels[comp_q_token] = [\"<\"+comp_q_token.strip(\"<>\")+\"_{}>\".format(i) for i in range(3)]\n",
    "    vocab = vocab + comp2labels[comp_q_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure uniqueness of new vocab\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_atomic(entity, attr, val, t):\n",
    "    val = \"<{}>\".format(val)\n",
    "    input_text = \"\".join([\"<q>\", entity, attr, \"<mask>\", \"</q>\"])\n",
    "    target_text = input_text + \"\".join([\"<a>\", val, \"</a>\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text,\n",
    "        \"type\": t,\n",
    "    }\n",
    "\n",
    "def format_comp(comp_q_token, ent_1, ent_2, label, t):\n",
    "    input_text = \"\".join([comp_q_token, \"<q>\", ent_1, \"<mask>\", ent_2, \"</q>\"])\n",
    "    target_text = input_text + \"\".join([\"<a>\", label, \"</a>\"])\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text,\n",
    "        \"type\": t,\n",
    "    }\n",
    "\n",
    "num_id_entities_ratio = 0.9\n",
    "\n",
    "id_atomic_facts, ood_atomic_facts = [], []\n",
    "train_inferred_topleft, train_inferred_cross, test_inferred_ood, test_inferred_iid = [], [], [], []\n",
    "\n",
    "def compare_ent(ent_1, ent_2, attr):\n",
    "    val_1, val_2 = atomic_KB[entity2ind[ent_1], attribute2ind[attr]], atomic_KB[entity2ind[ent_2], attribute2ind[attr]]\n",
    "    return compare(\"<{}>\".format(val_1), \"<{}>\".format(val_2))\n",
    "\n",
    "for comp_q_token in tqdm(comp_q_tokens):\n",
    "    # randomly partition the entities\n",
    "    id_entities, ood_entities = split(entities, num_id_entities_ratio)\n",
    "\n",
    "    # all attribute values\n",
    "    for entity in id_entities:\n",
    "        val = atomic_KB[entity2ind[entity], attribute2ind[comp_q_token]]\n",
    "        id_atomic_facts.append(format_atomic(entity, comp_q_token, val, t='id_atomic'))\n",
    "\n",
    "    for entity in ood_entities:\n",
    "        val = atomic_KB[entity2ind[entity], attribute2ind[comp_q_token]]\n",
    "        ood_atomic_facts.append(format_atomic(entity, comp_q_token, val, t='ood_atomic'))\n",
    "    \n",
    "    # add all pairs of entities for all\n",
    "    all_pairs = list(itertools.combinations(entities, 2))\n",
    "    for (ent_1, ent_2) in all_pairs:\n",
    "        if ent_1 in ood_entities and ent_2 in ood_entities:\n",
    "            ty = 'test_inferred_ood'\n",
    "            label = comp2labels[comp_q_token][compare_ent(ent_1, ent_2, comp_q_token)]\n",
    "            test_inferred_ood.append(format_comp(comp_q_token, ent_1, ent_2, label, t=ty))\n",
    "            # flip\n",
    "            label = comp2labels[comp_q_token][compare_ent(ent_2, ent_1, comp_q_token)]\n",
    "            test_inferred_ood.append(format_comp(comp_q_token, ent_2, ent_1, label, t=ty))\n",
    "        elif ent_1 in id_entities and ent_2 in id_entities:\n",
    "            if np.random.uniform() < 0.1:\n",
    "                ty = 'test_inferred_iid'\n",
    "                label = comp2labels[comp_q_token][compare_ent(ent_1, ent_2, comp_q_token)]\n",
    "                test_inferred_iid.append(format_comp(comp_q_token, ent_1, ent_2, label, t=ty))\n",
    "                # flip\n",
    "                label = comp2labels[comp_q_token][compare_ent(ent_2, ent_1, comp_q_token)]\n",
    "                test_inferred_iid.append(format_comp(comp_q_token, ent_2, ent_1, label, t=ty))\n",
    "            else:\n",
    "                ty = 'train_inferred_topleft'\n",
    "                label = comp2labels[comp_q_token][compare_ent(ent_1, ent_2, comp_q_token)]\n",
    "                train_inferred_topleft.append(format_comp(comp_q_token, ent_1, ent_2, label, t=ty))\n",
    "                # flip\n",
    "                label = comp2labels[comp_q_token][compare_ent(ent_2, ent_1, comp_q_token)]\n",
    "                train_inferred_topleft.append(format_comp(comp_q_token, ent_2, ent_1, label, t=ty))\n",
    "        else:\n",
    "            ty = 'train_inferred_cross'\n",
    "            label = comp2labels[comp_q_token][compare_ent(ent_1, ent_2, comp_q_token)]\n",
    "            train_inferred_cross.append(format_comp(comp_q_token, ent_1, ent_2, label, t=ty))\n",
    "            # flip\n",
    "            label = comp2labels[comp_q_token][compare_ent(ent_2, ent_1, comp_q_token)]\n",
    "            train_inferred_cross.append(format_comp(comp_q_token, ent_2, ent_1, label, t=ty))\n",
    "\n",
    "print(len(id_atomic_facts), len(ood_atomic_facts), \"|\", len(train_inferred_topleft), len(train_inferred_cross), len(test_inferred_ood), len(test_inferred_iid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = []\n",
    "\n",
    "test_size = 3000\n",
    "comp_facts_test_ds = choose(test_inferred_ood, test_size)\n",
    "probes = probes + comp_facts_test_ds\n",
    "\n",
    "probes = probes + choose(id_atomic_facts, test_size)\n",
    "probes = probes + choose(test_inferred_iid, test_size)\n",
    "probes = probes + ood_atomic_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling inferred facts included in training\n",
    "train_downsampling = 0.03\n",
    "dataset_name = \"cplx_reasoning\"     # \n",
    "os.makedirs(\"data/{}\".format(dataset_name), exist_ok=True)\n",
    "\n",
    "train_inferred_topleft_ds = choose(train_inferred_topleft, train_downsampling)\n",
    "train_inferred_cross_ds = choose(train_inferred_cross, train_downsampling)\n",
    "\n",
    "probes = probes + choose(train_inferred_topleft_ds, test_size)\n",
    "probes = probes + choose(train_inferred_cross_ds, test_size)\n",
    "\n",
    "with open(\"data/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(id_atomic_facts + train_inferred_topleft_ds + train_inferred_cross_ds, f)\n",
    "with open(\"data/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(comp_facts_test_ds, f)\n",
    "with open(\"data/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the test set\n",
    "from itertools import permutations\n",
    "\n",
    "with open(\"data/{}/train.json\".format(dataset_name)) as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "train_dict = dict()\n",
    "all_atomics = dict()\n",
    "for item in tqdm(train):\n",
    "    t = item['type']\n",
    "    attr = int(item['input_text'].split(\"attr_\")[1].split(\">\")[0])\n",
    "    assert 0 <= attr <= 19\n",
    "    if t not in train_dict:\n",
    "        train_dict[t] = dict()\n",
    "    if attr not in train_dict[t]:\n",
    "        train_dict[t][attr] = []\n",
    "    train_dict[t][attr].append(item)\n",
    "\n",
    "# (attr, e1, e2, label)\n",
    "solvable_dict = set()   \n",
    "for attr in tqdm(range(20)):\n",
    "\n",
    "    train_inferred_cross = train_dict['train_inferred_cross'][attr]\n",
    "    G = nx.DiGraph()   # a->b iff a>=b\n",
    "    for item in train_inferred_cross:\n",
    "        temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "        e1, e2, label = temp[2], temp[4], int(temp[7][-1])\n",
    "        if label == 2:\n",
    "            G.add_edge(e1, e2)\n",
    "        if label == 1:\n",
    "            G.add_edge(e1, e2)\n",
    "            G.add_edge(e2, e1)\n",
    "        if label == 0:\n",
    "            G.add_edge(e2, e1)\n",
    "\n",
    "    train_atomic = train_dict['id_atomic'][attr]\n",
    "    atomics = dict()\n",
    "    for item in train_atomic:\n",
    "        temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "        e, val = temp[1], temp[6]\n",
    "        atomics[e] = int(val)\n",
    "        assert 0 <= atomics[e] <= 19\n",
    "\n",
    "    ents = list(atomics.keys())\n",
    "    for i in range(len(ents)-1):\n",
    "        for j in range(i+1, len(ents)):\n",
    "            e1, e2 = ents[i], ents[j]\n",
    "            if atomics[e1] > atomics[e2]:\n",
    "                G.add_edge(e1, e2)\n",
    "            if atomics[e1] == atomics[e2]:\n",
    "                G.add_edge(e1, e2)\n",
    "                G.add_edge(e2, e1)\n",
    "            if atomics[e1] < atomics[e2]:\n",
    "                G.add_edge(e2, e1)\n",
    "    all_atomics[attr] = atomics\n",
    "\n",
    "    all_nodes = set(G.nodes)\n",
    "    ood_nodes = list(all_nodes - set(atomics.keys()))\n",
    "    assert len(ood_nodes) == 100\n",
    "    # Testing\n",
    "    for (e1, e2) in list(permutations(ood_nodes, 2)):\n",
    "        greater_or_equal = int(nx.has_path(G, e1, e2))\n",
    "        smaller_or_equal = int(nx.has_path(G, e2, e1))\n",
    "        if greater_or_equal or smaller_or_equal:\n",
    "            if greater_or_equal and smaller_or_equal:\n",
    "                res = 1\n",
    "            elif greater_or_equal:\n",
    "                res = 2\n",
    "            elif smaller_or_equal:\n",
    "                res = 0\n",
    "            solvable_dict.add((attr, e1, e2, res))\n",
    "\n",
    "easy_dict = set()\n",
    "for attr in tqdm(range(20)):\n",
    "\n",
    "    train_inferred = train_dict['train_inferred_cross'][attr] + train_dict['train_inferred_topleft'][attr]\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for item in train_inferred:\n",
    "        temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "        e1, e2, label = temp[2], temp[4], int(temp[7][-1])\n",
    "        if label == 2:\n",
    "            G.add_edge(e1, e2)\n",
    "        if label == 1:\n",
    "            G.add_edge(e1, e2)\n",
    "            G.add_edge(e2, e1)\n",
    "        if label == 0:\n",
    "            G.add_edge(e2, e1)\n",
    "\n",
    "    train_atomic = train_dict['id_atomic'][attr]\n",
    "    atomics = dict()\n",
    "    for item in train_atomic:\n",
    "        temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "        e, val = temp[1], temp[6]\n",
    "        atomics[e] = int(val)\n",
    "        assert 0 <= atomics[e] <= 19\n",
    "\n",
    "    all_nodes = set(G.nodes)\n",
    "    ood_nodes = list(all_nodes - set(atomics.keys()))\n",
    "    assert len(ood_nodes) == 100\n",
    "    # Testing\n",
    "    for (e1, e2) in list(permutations(ood_nodes, 2)):\n",
    "        greater_or_equal = int(nx.has_path(G, e1, e2))\n",
    "        smaller_or_equal = int(nx.has_path(G, e2, e1))\n",
    "        if greater_or_equal or smaller_or_equal:\n",
    "            if greater_or_equal and smaller_or_equal:\n",
    "                res = 1\n",
    "            elif greater_or_equal:\n",
    "                res = 2\n",
    "            elif smaller_or_equal:\n",
    "                res = 0\n",
    "            easy_dict.add((attr, e1, e2, res))\n",
    "\n",
    "\n",
    "solvable_hard_set = solvable_dict - easy_dict\n",
    "print(len(solvable_hard_set))\n",
    "\n",
    "group_by_label = dict()\n",
    "for g in solvable_hard_set:\n",
    "    key = g[-1]\n",
    "    if key not in group_by_label:\n",
    "        group_by_label[key] = []\n",
    "    group_by_label[key].append(g)\n",
    "\n",
    "test_hard = choose(group_by_label[0], 50) + choose(group_by_label[1], 50) + choose(group_by_label[2], 50)\n",
    "test = []\n",
    "for (attr, e1, e2, label) in test_hard:\n",
    "    test.append(\n",
    "        format_comp(\"<attr_{}>\".format(attr), \"<\"+e1+\">\", \"<\"+e2+\">\", \"<attr_{}_{}>\".format(attr, label), 'test_hard'))\n",
    "\n",
    "\n",
    "probes = probes + test\n",
    "with open(\"data/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(probes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
